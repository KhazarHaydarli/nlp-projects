# -*- coding: utf-8 -*-
# """Speech_recognition.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/13vkRxFFbouAllGhpdy43P3jnNW_4bAzJ
# """

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install datasets==1.4.1
# !pip install transformers==4.4.0
# !pip install torchaudio
# !pip install librosa
# !pip install jiwer
# # !pip install urduhack

# from google.colab import drive
# drive.mount('/content/drive')

"""## Data Preprocessing, Tokenizer, Feature Extractor"""

from datasets import Dataset
import pandas as pd
import numpy as np
from datasets import load_metric
# from urduhack.preprocessing import replace_numbers
# from urduhack.preprocessing import normalize_whitespace
# from urduhack.preprocessing import remove_english_alphabets
# from urduhack.preprocessing import remove_accents
# from urduhack.preprocessing import remove_punctuation

"""Data Preprocessing using Urduhack

*   Train Data
"""

data = pd.read_csv("train_.csv")
# data_cleaned = []

# for i in data['text']:
#   x = remove_punctuation(i)
#   x = replace_numbers(i)
#   x = normalize_whitespace(i)
#   x = remove_english_alphabets(x)
#   x = remove_accents(x)

#   data_cleaned.append(x)

# data['text'] = data_cleaned
data

"""
*   Test Data

"""

test_data = pd.read_csv("test_.csv")
test_data

# SITE_PACKAGES_FOLDER = !(python3 -c "import sysconfig; print(sysconfig.get_paths()['purelib'])")
# ! echo SITE_PACKAGES_FOLDER

# ! ls $SITE_PACKAGES_FOLDER/numpy*

# ! pip install trash-cli
# ! trash $SITE_PACKAGES_FOLDER/numpy*

# ! pip install --upgrade numpy

import os.path
key = []
text = []

for x in range(len(data['audio'])):
  # if os.path.isfile(data['audio'][x]):
  key.append(data['audio'][x])
  text.append(data['text'][x])

data = {'key' : key, 'text' : text}
data = pd.DataFrame(data)
data = Dataset.from_pandas(data)
data

key = []
text = []
for x in range(len(test_data['audio'])):
  key.append(test_data['audio'][x])
  text.append(test_data['text'][x])

test_data = {'key' : key, 'text' : text}
test_data = pd.DataFrame(test_data)
test_data = Dataset.from_pandas(test_data)
test_data

"""Displaying Random samples"""

from datasets import ClassLabel
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    display(HTML(df.to_html()))

show_random_elements(data.remove_columns(['key']))

def extract_all_chars(batch):
  all_text = " ".join(batch["text"])
  vocab = list(set(all_text))
  return {"vocab": [vocab], "all_text": [all_text]}

vocab_dict = {
0: ' ',
1: 'q',
2: 'w',
3: 'e',
4: 'r',
5: 't',
6: 'y',
7: 'u',
8: 'i',
9: 'o',
10: 'p',
11: 'a',
12: 's',
13: 'd',
14: 'f',
15: 'g',
16: 'h',
17: 'j',
18: 'k',
19: 'l',
20: 'z',
21: 'x',
22: 'c',
23: 'v',
24: 'b',
25: 'n',
26: 'm',
# 27: '',
# 28: 'ق',
# 29: 'ک',
# 30: 'گ',
# 31: 'ل',
# 32: 'م',
# 33: 'ن',
# 34: 'ں',
# 35: 'و',
# 36: 'ہ',
# 37: 'ھ',
# 38: 'ء',
# 39: 'ئ',
# 40: 'ی',
# 41: 'ے'
}

vocab_dict["[UNK]"] = len(vocab_dict)
vocab_dict["[PAD]"] = len(vocab_dict)
len(vocab_dict)

import json
with open('vocab.json', 'w') as vocab_file:
    json.dump(vocab_dict, vocab_file)

from transformers import Wav2Vec2CTCTokenizer

tokenizer = Wav2Vec2CTCTokenizer("./vocab.json", unk_token="[UNK]", pad_token="[PAD]", word_delimiter_token="|")

from transformers import Wav2Vec2FeatureExtractor

feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)

"""### Create XLSR-Wav2Vec2 Feature Extractor"""

from transformers import Wav2Vec2Processor

processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)

"""### Preprocess Data

So far, we have not looked at the actual values of the speech signal but just kept the path to its file in the dataset. `XLSR-Wav2Vec2` expects the audio file in the format of a 1-dimensional array, so in the first step, let's load all audio files into the dataset object.

Let's first check the serialization format of the downloaded audio files by looking at the first training sample.
"""

import torchaudio

def speech_file_to_array_fn(batch):
    speech_array, sampling_rate = torchaudio.load(batch["key"])
    batch["speech"] = speech_array[0].numpy()
    batch["sampling_rate"] = sampling_rate
    batch["target_text"] = batch["text"]
    return batch

train_set = data.map(speech_file_to_array_fn, remove_columns=data.column_names)
test_set = test_data.map(speech_file_to_array_fn, remove_columns=test_data.column_names)

import librosa
import numpy as np

def resample(batch):
    batch["speech"] = librosa.resample(np.asarray(batch["speech"]), 16_000, 16_000)
    batch["sampling_rate"] = 16_000
    return batch

train_set = train_set.map(resample, num_proc=4)
test_set = test_set.map(resample, num_proc=4)

import IPython.display as ipd
import numpy as np
import random

rand_int = random.randint(0, len(train_set)-1)

ipd.Audio(data=np.asarray(train_set[rand_int]["speech"]), autoplay=True, rate=16000)

# rand_int = random.randint(0, len(train_set)-1)

print("Target text:", train_set[rand_int]["target_text"])
print("Input array shape:", np.asarray(train_set[rand_int]["speech"]).shape)
print("Sampling rate:", train_set[rand_int]["sampling_rate"])

def prepare_dataset(batch):
    # check that all files have the correct sampling rate
    assert (
        len(set(batch["sampling_rate"])) == 1
    ), f"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}."

    batch["input_values"] = processor(batch["speech"], sampling_rate=batch["sampling_rate"][0]).input_values
    
    with processor.as_target_processor():
        batch["labels"] = processor(batch["target_text"]).input_ids
    return batch

train_set = train_set.map(prepare_dataset, remove_columns=train_set.column_names, batch_size=8, num_proc=4, batched=True)
test_set = test_set.map(prepare_dataset, remove_columns=test_set.column_names, batch_size=8, num_proc=4, batched=True)

train_set

test_set

"""#Training"""

import torch

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union

@dataclass
class DataCollatorCTCWithPadding:

    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True
    max_length: Optional[int] = None
    max_length_labels: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    pad_to_multiple_of_labels: Optional[int] = None

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # split inputs and labels since they have to be of different lenghts and need
        # different padding methods
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )
        with self.processor.as_target_processor():
            labels_batch = self.processor.pad(
                label_features,
                padding=self.padding,
                max_length=self.max_length_labels,
                pad_to_multiple_of=self.pad_to_multiple_of_labels,
                return_tensors="pt",
            )

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        batch["labels"] = labels

        return batch

data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)

wer_metric = load_metric("wer")

def compute_metrics(pred):
    pred_logits = pred.predictions
    pred_ids = np.argmax(pred_logits, axis=-1)

    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.batch_decode(pred_ids)
    # we do not want to group tokens when computing the metrics
    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

    wer = wer_metric.compute(predictions=pred_str, references=label_str)

    return {"wer": wer}

from transformers import Wav2Vec2ForCTC

model = Wav2Vec2ForCTC.from_pretrained(
    "facebook/wav2vec2-large-xlsr-53", 
    attention_dropout=0.1,
    hidden_dropout=0.1,
    feat_proj_dropout=0.0,
    mask_time_prob=0.05,
    layerdrop=0.1,
    gradient_checkpointing=True, 
    ctc_loss_reduction="mean", 
    pad_token_id=processor.tokenizer.pad_token_id,
    vocab_size=len(processor.tokenizer)
)

model.freeze_feature_extractor()

from transformers import TrainingArguments

training_args = TrainingArguments(
  output_dir="/content/gdrive/MyDrive/wav2vec2-urdu",
  # output_dir="./wav2vec2-large-xlsr-turkish-demo",
  group_by_length=True,
  per_device_train_batch_size=16,
  gradient_accumulation_steps=2,
  evaluation_strategy="steps",
  num_train_epochs=30,
  fp16=True,
  save_steps=400,
  eval_steps=400,
  logging_steps=400,
  learning_rate=3e-4,
  warmup_steps=500,
  save_total_limit=2,
)

data = train_set.train_test_split(test_size=0.2)
data['train']
data['test']

from transformers import Trainer

trainer = Trainer(
    model=model,
    data_collator=data_collator,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=data['train'],
    eval_dataset=data['test'],
    tokenizer=processor.feature_extractor,
)

trainer.train()

model = Wav2Vec2ForCTC.from_pretrained("/content/gdrive/MyDrive/wav2vec2-urdu/checkpoint-800").to("cuda")
processor = Wav2Vec2Processor.from_pretrained("/content/gdrive/MyDrive/wav2vec2-urdu/checkpoint-800")

input_dict = processor(test_set[0]["input_values"], return_tensors="pt", padding=True)
logits = model(input_dict.input_values.to("cuda")).logits
pred_ids = torch.argmax(logits, dim=-1)[0]

print("Prediction:")
print(processor.decode(pred_ids))

print("\nReference:")
print(test_set["labels"])

